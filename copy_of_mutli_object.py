# -*- coding: utf-8 -*-
"""Copy of mutli object.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L8nlzdK5RI0lMHMkqhQsHuNAI3eOO06t
"""

# Commented out IPython magic to ensure Python compatibility.
# Clone the YOLOv5 repository
!git clone https://github.com/ultralytics/yolov5.git
# %cd yolov5

# Install dependencies
!pip install -U -r requirements.txt

!pip install streamlit



# Download a pre-trained YOLOv5 model (e.g., YOLOv5s)
!python detect.py --source https://ultralytics.com/images/zidane.jpg --weights yolov5s.pt --img-size 640

# Create the Streamlit app file (app.py)
app_code = """
import streamlit as st
from PIL import Image
import torch
import io

# Title of the app
st.title('YOLOv5 Object Detection with Streamlit')

# File uploader for image or video
uploaded_file = st.file_uploader("Upload an image or video", type=["jpg", "jpeg", "png", "mp4"])

# Check if the file is uploaded
if uploaded_file is not None:
    # Read the image or video
    img_bytes = uploaded_file.read()

    # For images, use PIL to display
    if uploaded_file.type in ["image/jpeg", "image/png", "image/jpg"]:
        st.image(img_bytes, caption="Uploaded Image", use_column_width=True)

        # Convert bytes to a PIL Image
        img = Image.open(io.BytesIO(img_bytes))

        # Run YOLOv5 inference
        model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLOv5 model (small version)
        results = model(img)

        # Display results
        st.write("Detected Objects:")
        st.write(results.pandas().xywh)

        # Show the output image with bounding boxes
        st.image(results.render()[0], caption="Processed Image", use_column_width=True)

    # For videos
    elif uploaded_file.type == "video/mp4":
        st.video(uploaded_file)
        # Here you could add more code to process the video with YOLOv5, but it requires more advanced handling
"""

# Write the code to a file named app.py
with open("/content/yolov5/app.py", "w") as file:
    file.write(app_code)

print("Streamlit app file 'app.py' has been created!")

# Run the Streamlit app
!streamlit run /content/yolov5/app.py & npx localtunnel --port 8501

import streamlit as st
from PIL import Image
import torch
import io
import cv2
import numpy as np
import tempfile

# Title of the app
st.title('Multi Object Detection')

# File uploader for image or video
uploaded_file = st.file_uploader("Upload an image or video", type=["jpg", "jpeg", "png", "mp4"])

# Load YOLOv5 model (large version for better accuracy)
model = torch.hub.load('ultralytics/yolov5', 'yolov5m')  # Use 'yolov5m' for better detection

# Check if the file is uploaded
if uploaded_file is not None:
    # Read the image or video
    img_bytes = uploaded_file.read()

    # For images, use PIL to display and process
    if uploaded_file.type in ["image/jpeg", "image/png", "image/jpg"]:
        # Display the uploaded image
        st.image(img_bytes, caption="Uploaded Image", use_container_width=True)

        # Convert bytes to a PIL Image
        img = Image.open(io.BytesIO(img_bytes))

        # Run YOLOv5 inference on the image
        results = model(img)

        # Display results
        st.write("Detected Objects:")
        st.write(results.pandas().xywh)  # Show bounding box information

        # Show the output image with bounding boxes
        st.image(results.render()[0], caption="Processed Image", use_container_width=True)

    # For videos, process frame by frame
    elif uploaded_file.type == "video/mp4":
        # Save the uploaded video to a temporary file
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            temp_file.write(img_bytes)
            temp_file_path = temp_file.name

        # Show the video to the user
        st.video(temp_file_path)

        # Read the video file using OpenCV
        cap = cv2.VideoCapture(temp_file_path)
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # Perform inference on the current frame
            results = model(frame)

            # Render the results on the frame (bounding boxes)
            frame_with_boxes = results.render()[0]

            # Convert frame with boxes to RGB format for display in Streamlit
            frame_rgb = cv2.cvtColor(frame_with_boxes, cv2.COLOR_BGR2RGB)

            # Show the processed frame
            st.image(frame_rgb, channels="RGB", use_container_width=True)

        cap.release()

!wget -q -O - https://loca.lt/mytunnelpassword